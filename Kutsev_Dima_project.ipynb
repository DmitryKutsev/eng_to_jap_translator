{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled47.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRp8xxL0pJxFkERdl8DJ7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/eng_to_jap_translator/blob/main/Kutsev_Dima_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnQDBCr5XOo-"
      },
      "source": [
        "**Neural Machine Translation.**\n",
        "\n",
        "Проект - модель генерации перевода с английского на японский. В основе лежит модель seq2seq, но если получится, то я попробую немного усложнить в дальнейшем.\n",
        "\n",
        "https://arxiv.org/pdf/1706.08198.pdf , \n",
        "https://www.aclweb.org/anthology/W14-7008.pdf - статьи, на которые примерно ориентировался.\n",
        "\n",
        "В качестве токенизатора японского языка использовал tinysegmenter: https://pypi.org/project/tinysegmenter/.\n",
        "\n",
        "Данные:\n",
        "\n",
        "Сначала был корпус Kurohashi-Kawahara Lab:  http://nlp.ist.i.kyoto-u.ac.jp/EN/?JEC%20Basic%20Sentence%20Data\n",
        "\n",
        "\n",
        "Потом нашел корпус побольше, параллельный корпус англо-японских субтитров https://nlp.stanford.edu/projects/jesc/data/raw.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ytt4JlBZO3M",
        "outputId": "224b4c47-4efd-4f33-8f66-bd83557482ca"
      },
      "source": [
        "!pip install tinysegmenter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tinysegmenter in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F_2aqNlVxoX"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "import random\n",
        "import spacy\n",
        "import tinysegmenter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3u2k5TuXBb"
      },
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1Np6TNtVY9"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-t9olLUZ11J"
      },
      "source": [
        "segmenter = tinysegmenter.TinySegmenter()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSu6zEknVPdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18645ac1-30fd-46fb-e1af-04847f89cc14"
      },
      "source": [
        "! wget https://nlp.stanford.edu/projects/jesc/data/raw.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-21 21:33:38--  https://nlp.stanford.edu/projects/jesc/data/raw.tar.gz\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102198198 (97M) [application/x-gzip]\n",
            "Saving to: ‘raw.tar.gz.2’\n",
            "\n",
            "raw.tar.gz.2        100%[===================>]  97.46M  2.99MB/s    in 39s     \n",
            "\n",
            "2020-12-21 21:34:17 (2.51 MB/s) - ‘raw.tar.gz.2’ saved [102198198/102198198]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0EV_NgMVSMh"
      },
      "source": [
        "!tar -xzf raw.tar.gz"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-TWmtS-VWVs"
      },
      "source": [
        "my_frame = pd.read_csv('raw/raw', sep='\\t')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vAkhV_XVgK"
      },
      "source": [
        "\n",
        "my_frame.columns = ['en', 'jp']\n",
        "my_frame = my_frame[:500000]\n",
        "# my_frame = my_frame[my_frame.columns[::-1]]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOfXTtcClM7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "52321885-32e4-42c7-8b88-15813243cc7c"
      },
      "source": [
        "my_frame"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>jp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my opponent is shark.</td>\n",
              "      <td>俺の相手は シャークだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this is one thing in exchange for another.</td>\n",
              "      <td>引き換えだ ある事とある物の</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yeah, i'm fine.</td>\n",
              "      <td>もういいよ ごちそうさま ううん</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>don't come to the office anymore. don't call m...</td>\n",
              "      <td>もう会社には来ないでくれ 電話もするな</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>looks beautiful.</td>\n",
              "      <td>きれいだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499995</th>\n",
              "      <td>i was threatened by a guy from your office.</td>\n",
              "      <td>FBIの男に強要されたのに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499996</th>\n",
              "      <td>it's distracting.</td>\n",
              "      <td>シャーペン回すんやめてくれへん? 気が散んねん。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499997</th>\n",
              "      <td>it provides a simple, inexpensive</td>\n",
              "      <td>荒れた生態系に水を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>i've talked to every morgue attendant, every m...</td>\n",
              "      <td>全ての職員や運転手と話を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>there's no way he was dirty.</td>\n",
              "      <td>彼が汚かったことはない</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       en                        jp\n",
              "0                                   my opponent is shark.              俺の相手は シャークだ。\n",
              "1              this is one thing in exchange for another.            引き換えだ ある事とある物の\n",
              "2                                         yeah, i'm fine.          もういいよ ごちそうさま ううん\n",
              "3       don't come to the office anymore. don't call m...       もう会社には来ないでくれ 電話もするな\n",
              "4                                        looks beautiful.                     きれいだ。\n",
              "...                                                   ...                       ...\n",
              "499995        i was threatened by a guy from your office.             FBIの男に強要されたのに\n",
              "499996                                  it's distracting.  シャーペン回すんやめてくれへん? 気が散んねん。\n",
              "499997                  it provides a simple, inexpensive                 荒れた生態系に水を\n",
              "499998  i've talked to every morgue attendant, every m...              全ての職員や運転手と話を\n",
              "499999                       there's no way he was dirty.               彼が汚かったことはない\n",
              "\n",
              "[500000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "867hWid-lsCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e56a80-9d99-4d3c-f0f2-641c255db769"
      },
      "source": [
        "segmenter.tokenize(my_frame['jp'][1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['引き換え', 'だ', ' ', 'ある', '事', 'と', 'ある', '物', 'の']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZsQH33qt4Sf",
        "outputId": "4797423c-a533-4a0b-b50e-3fbc3d92a418"
      },
      "source": [
        "[tok.text for tok in spacy_en.tokenizer(my_frame['en'][1])]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'one', 'thing', 'in', 'exchange', 'for', 'another', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiMWnSpXt4Yc"
      },
      "source": [
        "my_frame.to_csv('my_frame.csv', index=False)  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2SGJc8cg7M"
      },
      "source": [
        "В какой-то момент я случайно убрал MAX_LEN, о чем забыл потом, и некоторое время не мог понять, почему на полных данных модель падает."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZu4XaC6u2ON"
      },
      "source": [
        "# MAX_LEN = 25\n",
        "MAX_LEN = 15\n",
        "\n",
        "def tokenize_jp(text):\n",
        "    \"\"\"\n",
        "    Tokenizes JP text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    stops = ['  ', ' ', '...',  '「',  '、', '」', '➡',  '《', '-', '「']\n",
        "    return [i for i in segmenter.tokenize(text)[:MAX_LEN] if i not in stops]\n",
        "\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    res = [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "    return res[:MAX_LEN]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM14NJ7Vc3G7"
      },
      "source": [
        "В качестве обработчика данных решил использовать инструменты torchtext.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBmfATqXvEdh"
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')\n",
        "TRG = Field(tokenize=tokenize_jp, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDjF--svKCa"
      },
      "source": [
        "dataset = TabularDataset(path='my_frame.csv', \n",
        "                         format='csv', \n",
        "                         fields=[ ('en', SRC), ('jp', TRG)],\n",
        "                         skip_header=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb_ZTRXCvXvT"
      },
      "source": [
        "train_data, valid_data, test_data = dataset.split(split_ratio=[0.7, 0.1, 0.2], \n",
        "                                            random_state=random.getstate())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R_NsUacaOI"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=1)\n",
        "TRG.build_vocab(train_data, min_freq=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MAiAqTUA7nN",
        "outputId": "099ec485-a030-473a-eaa7-82508939709a"
      },
      "source": [
        "print(vars(train_data.examples[0])['en'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['you', \"'re\", 'good', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-ZYWryrI8kJ",
        "outputId": "7b6393d6-91f7-4213-83b2-be0f4303bd7e"
      },
      "source": [
        "print(vars(train_data.examples[0])['jp'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['うまい', 'なぁ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmJTfi3NveB3",
        "outputId": "cba2af24-cc91-4ae4-f7ff-82a150771d0f"
      },
      "source": [
        "print (len(SRC.vocab), len(TRG.vocab))\n",
        "print (SRC.vocab.freqs.most_common(10))\n",
        "print (TRG.vocab.freqs.most_common(10))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56956 109573\n",
            "[('.', 212748), (',', 117190), ('you', 86354), ('the', 85398), ('i', 82518), ('?', 63348), ('to', 60081), ('a', 50291), (\"'s\", 48697), ('it', 48531)]\n",
            "[('の', 129116), ('は', 97673), ('に', 86992), ('て', 79552), ('を', 74538), ('が', 69041), ('た', 65326), ('?', 48749), ('だ', 43575), ('で', 42941)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfcvmRLPvh7J"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_key=lambda x: len(x.jp), \n",
        "     sort_within_batch=False,\n",
        "     device = device)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4ijkRfBLlfE"
      },
      "source": [
        "\n",
        "# for b in valid_iterator:\n",
        "#     print (b.jp, b.en)\n",
        "#     sys.exit()\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIUixh0j_4I7",
        "outputId": "d41f2060-cfdf-4e56-dec6-aca0dcab8e96"
      },
      "source": [
        "print(vars(valid_iterator))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 256, 'train': False, 'dataset': <torchtext.data.dataset.Dataset object at 0x7f4c9e34dc50>, 'batch_size_fn': None, 'iterations': 0, 'repeat': False, 'shuffle': False, 'sort': True, 'sort_within_batch': False, 'sort_key': <function <lambda> at 0x7f4cc98e8378>, 'device': device(type='cpu'), 'random_shuffler': <torchtext.data.utils.RandomShuffler object at 0x7f4c9e3645f8>, '_iterations_this_epoch': 0, '_random_state_this_epoch': None, '_restored_from_state': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KERuieLSKp",
        "outputId": "13e31fe1-3ab5-492d-9b88-edf01714ab4f"
      },
      "source": [
        "print(len(valid_iterator))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5WmjNVHdJLm"
      },
      "source": [
        "Раздел с самими, собственно, сетями."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUIsuJTMxazl"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f1KFSFe755Z"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ouCUie7-Yq"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            \n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1) \n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8DFIOPOBiWh",
        "outputId": "5b1d0372-99c0-4966-836f-5c40b3d12220"
      },
      "source": [
        "len(SRC.vocab), len(TRG.vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56956, 109573)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhRW4pSI8B33"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.4\n",
        "DEC_DROPOUT = 0.4\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnWCIOb28Eca",
        "outputId": "a5b57d93-472d-480f-dd21-86c93e657d4d"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(56956, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.4)\n",
              "    (dropout): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(109573, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.4)\n",
              "    (fc_out): Linear(in_features=512, out_features=109573, bias=True)\n",
              "    (dropout): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ9yJdDi8HQk",
        "outputId": "5590bbde-408b-463e-aae7-1c21694fd3a1"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 106,198,789 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NostO82q8KUa",
        "outputId": "3ac84c8b-263c-41f1-bf8f-d3d09ba92cbb"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "pad_idx = TRG.vocab.stoi['<pad>']\n",
        "print(TRG.pad_token)  # <pad>\n",
        "print(TRG.vocab.stoi[TRG.pad_token]) # 1 \n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad>\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4TAaoFsdi3u"
      },
      "source": [
        "import time"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU1wGUyCdsup"
      },
      "source": [
        "Обучение и оценка.\n",
        "\n",
        "Сначала я поставил progress_bar, и вроде избавился от всех его глюков в колабе, но но в какой-то момент он опять начал странно работать, и т.к. времени почти не осталось, я решил, что пока ограничусь выводом результатов в принт, а progress_bar вернется, если останется время его опять чинить.\n",
        "Так же включил ограничение по времени, последнее ограничение на обучение было 400 минут.\n",
        "(я пытался обучать дольше, но колаб, кажется, при некотором времени бездействия, просто отключается, поэтому просто на ночь оставлять не вышло).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ41zk0j8ZL8"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, start_time):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    my_losses = []\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # progress_bar = tqdm(total=len(iterator), desc=f'{ i }')\n",
        "        src = batch.en\n",
        "        trg = batch.jp\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        my_losses.append(loss.item())\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        iter_mins, iter_secs = epoch_time(start_time, end_time)\n",
        "        if i%10 == 0:\n",
        "          print(f'fmean losses: { np.mean(my_losses[-1000:]) } ', \n",
        "                f'iter { i }, iter mins { iter_mins }' )\n",
        "        if int(iter_mins) > 400:\n",
        "          return epoch_loss / len(iterator)\n",
        "        # progress_bar.set_postfix(loss=np.mean(my_losses[-1000:]),\n",
        "                            # perplexity=np.exp(np.mean(my_losses[-1000:])))\n",
        "        # progress_bar.update()\n",
        "     # progress_bar.close()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo20BXfSF4lg"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.en\n",
        "            trg = batch.jp\n",
        "\n",
        "            output = model(src, trg, 0) \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35jnuevHF6zH"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uz8gbaRyqlp"
      },
      "source": [
        "import time"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu90fN4X3S_Y"
      },
      "source": [
        "\n",
        "# for instance in list(tqdm._instances):\n",
        "#   tqdm._decr_instances(instance)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx58AftzP4RQ",
        "outputId": "64988def-1347-444a-ab62-74833f2c00d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zddV9-FLeaL_"
      },
      "source": [
        "Проход по данным. Но сверху еще есть хардкод в виде ограничения по времени.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaoXwxnfF8xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ad66d5-7673-4a23-a203-584514bc9fc1"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "total_start_time = time.time()\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, total_start_time)\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fmean losses: 11.601455688476562  iter 0, iter mins 0\n",
            "fmean losses: 9.360088565132834  iter 10, iter mins 8\n",
            "fmean losses: 8.187040851229714  iter 20, iter mins 16\n",
            "fmean losses: 7.723426818847656  iter 30, iter mins 23\n",
            "fmean losses: 7.451544028956715  iter 40, iter mins 31\n",
            "fmean losses: 7.279507898816876  iter 50, iter mins 39\n",
            "fmean losses: 7.159851488519887  iter 60, iter mins 46\n",
            "fmean losses: 7.070481965239619  iter 70, iter mins 54\n",
            "fmean losses: 6.999135058603169  iter 80, iter mins 62\n",
            "fmean losses: 6.937422464182089  iter 90, iter mins 70\n",
            "fmean losses: 6.889661543440111  iter 100, iter mins 77\n",
            "fmean losses: 6.846861749081998  iter 110, iter mins 85\n",
            "fmean losses: 6.806571412677608  iter 120, iter mins 93\n",
            "fmean losses: 6.773665737559777  iter 130, iter mins 101\n",
            "fmean losses: 6.747186934694331  iter 140, iter mins 109\n",
            "fmean losses: 6.719250991644449  iter 150, iter mins 117\n",
            "fmean losses: 6.695732131507826  iter 160, iter mins 125\n",
            "fmean losses: 6.670178176366795  iter 170, iter mins 132\n",
            "fmean losses: 6.645319314292781  iter 180, iter mins 140\n",
            "fmean losses: 6.622491150002205  iter 190, iter mins 148\n",
            "fmean losses: 6.603118422019541  iter 200, iter mins 156\n",
            "fmean losses: 6.584918157749266  iter 210, iter mins 164\n",
            "fmean losses: 6.569056439723364  iter 220, iter mins 172\n",
            "fmean losses: 6.5510959707813345  iter 230, iter mins 180\n",
            "fmean losses: 6.532844375277951  iter 240, iter mins 187\n",
            "fmean losses: 6.518940610239706  iter 250, iter mins 195\n",
            "fmean losses: 6.503528845264537  iter 260, iter mins 203\n",
            "fmean losses: 6.490500147492243  iter 270, iter mins 211\n",
            "fmean losses: 6.477204007186075  iter 280, iter mins 219\n",
            "fmean losses: 6.464295639614878  iter 290, iter mins 227\n",
            "fmean losses: 6.452756568046899  iter 300, iter mins 235\n",
            "fmean losses: 6.441854864838039  iter 310, iter mins 244\n",
            "fmean losses: 6.430528646688966  iter 320, iter mins 252\n",
            "fmean losses: 6.419770083758766  iter 330, iter mins 259\n",
            "fmean losses: 6.410557106443164  iter 340, iter mins 267\n",
            "fmean losses: 6.399949594100995  iter 350, iter mins 275\n",
            "fmean losses: 6.388947309879715  iter 360, iter mins 283\n",
            "fmean losses: 6.378208814605548  iter 370, iter mins 291\n",
            "fmean losses: 6.369595712877008  iter 380, iter mins 299\n",
            "fmean losses: 6.36168570530689  iter 390, iter mins 307\n",
            "fmean losses: 6.353622748072903  iter 400, iter mins 315\n",
            "fmean losses: 6.343617780365213  iter 410, iter mins 323\n",
            "fmean losses: 6.334486501517035  iter 420, iter mins 331\n",
            "fmean losses: 6.327455148896043  iter 430, iter mins 339\n",
            "fmean losses: 6.318030965030869  iter 440, iter mins 347\n",
            "fmean losses: 6.31083432956704  iter 450, iter mins 355\n",
            "fmean losses: 6.303419575515383  iter 460, iter mins 363\n",
            "fmean losses: 6.296140645466539  iter 470, iter mins 371\n",
            "fmean losses: 6.29210264915736  iter 480, iter mins 379\n",
            "fmean losses: 6.286364492233805  iter 490, iter mins 387\n",
            "fmean losses: 6.280526339174982  iter 500, iter mins 395\n",
            "Epoch: 01 | Time: 401m 10s\n",
            "\tTrain Loss: 2.331 | Train PPL:  10.288\n",
            "fmean losses: 5.870341777801514  iter 0, iter mins 402\n",
            "Epoch: 02 | Time: 0m 54s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "fmean losses: 5.911101341247559  iter 0, iter mins 402\n",
            "Epoch: 03 | Time: 0m 54s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "fmean losses: 5.808311462402344  iter 0, iter mins 403\n",
            "Epoch: 04 | Time: 0m 54s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "fmean losses: 5.7386250495910645  iter 0, iter mins 404\n",
            "Epoch: 05 | Time: 0m 54s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh-VuR25gMxC"
      },
      "source": [
        "def translate_sentence(sentence,src_field,trg_field,model,device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence,str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "   \n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    return trg_tokens[1:-1] "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMrCC7JrgR6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4de6f30-4ca7-40a6-c339-c03ebd3aa7bf"
      },
      "source": [
        "example_idx = 24\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['en']\n",
        "trg = vars(train_data.examples[example_idx])['jp']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['you', 'know', ',', 'the', 'police', 'ca', \"n't\", 'act', 'unless', 'it', 'turns', 'into', 'a', 'real', 'incident']\n",
            "trg = ['でも', '。 ', '(', '谷村', ')', '警察', 'は', 'ね', '→']\n",
            "predicted trg = ['私', 'の', 'は', 'の', 'の', 'の', 'の', 'を']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0HkTcEQIrL"
      },
      "source": [
        "model_save_name = 'tut1-model.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4FKTMgMn_BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70ab154-9b92-4702-9c2e-3e113b4bec62"
      },
      "source": [
        "translation2 = translate_sentence('I go to school', SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = { \" \".join(translation2) } ')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = 私 は \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG1b4gY98R3d",
        "outputId": "a6b41e31-1204-44af-9acb-7af797f12376"
      },
      "source": [
        "translation2 = translate_sentence('I want to eat and drink', SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = { \" \".join(translation2) }')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = 私 の は を\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLAEyJe2ellX"
      },
      "source": [
        "По визуальной оценке результат совсем не очень. Если бы было немного больше времени, я бы добавил attention, и вообще попробовал бы transformer.\n",
        "Возможно, я даже успею что-то добавить(beam search, например). Если тут все еще этот текст - значит я не успел, или у меня не вышло.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glObHF7zlPsn"
      },
      "source": [
        "В качестве оценки использовал метрику bleu, а реализацию взял из библиотеки NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZR3kSGRiLrt"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate import bleu_score"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JglcBOBRaLTv",
        "outputId": "9f9523f1-746f-4bdb-98a1-3fcfd87a0a59"
      },
      "source": [
        "one = [['うまい', 'なぁ', 'は', 'jgjg', '111']]\n",
        "two = ['すべて', 'は', 'ここから', '始まっ', 'た']\n",
        "sentence_bleu(one, two, weights=(1, 0, 0, 0))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWwvW_sjgMHM",
        "outputId": "fe26a17e-ec43-4ded-b7fc-ea126b2e1c1e"
      },
      "source": [
        "for i in train_data.examples[:3]:\n",
        "  print(vars(i)['jp'])\n",
        "  #print(vars(i)['en'])\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['うまい', 'なぁ']\n",
            "['すべて', 'は', 'ここから', '始まっ', 'た']\n",
            "['そう', 'です', 'よ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ92j7FuTDLS"
      },
      "source": [
        "В качестве метрики использую метрику BLEU, импортирую из NLTK. Собираю список из целей в датасете, и список из переводов модели, и сравниваю.\n",
        "\n",
        "Прошлая функция подсчета оказалась не правильной, в статьях BLEU score набирал максимум 20, а у меня получалось 50-60.\n",
        "Все встает на свои места, если убрать расчет весов для н-грамм."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28m6on818d71"
      },
      "source": [
        "def cal_bleu_score(dataset_pairs):\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    for i in dataset_pairs:\n",
        "        target = vars(i)['jp']\n",
        "        target = ' '.join(target)\n",
        "        predicted_words = translate_sentence(vars(i)['en'], SRC, TRG, model, device)\n",
        "        predictions.append(' '.join(predicted_words))\n",
        "        targets.append(target)\n",
        "    print(predictions[:3])\n",
        "    print(targets[:3])\n",
        "    print(f'BLEU Score: {round(corpus_bleu(predictions, targets, weights=(1, 0, 0, 0)) * 100, 2)}')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv96m6eFj3nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f449e2c-8eb8-43d6-b70f-9674ada0193d"
      },
      "source": [
        "len(valid_data.examples)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TjhaUmPgc4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c2daa5-8cb0-4629-8adf-b90ba2c4b17c"
      },
      "source": [
        "cal_bleu_score(valid_data.examples)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['私 の は の の を', '何 は は', '私 の は']\n",
            "['ブラッド の 歯 の 型 を ベース に し て', 'あっ 何 すん だ よ', 'やが て スティーブ が 口 を 開き まし た']\n",
            "BLEU Score: 9.27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzAGNHZ9Z_RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe813c61-6499-41a0-e87f-0ffa5f21967b"
      },
      "source": [
        "cal_bleu_score(train_data.examples)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['何 は', '私 の は', '私 は は は']\n",
            "['うまい なぁ', 'すべて は ここから 始まっ た', 'そう です よ']\n",
            "BLEU Score: 8.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLjw-7lk8s3"
      },
      "source": [
        "В результате все не очень хорошо.\n",
        "Судя по статьям, нужно гораздо дольше обучать, добавлять контекстный вектор Attention и beam search как минимум.\n",
        "\n",
        "Что еще убило огромную кучу времени - это колаб. Он внезапно падал, терял соединение, не сохранял блокноты, в общем да, его я постараюсь больше не использовать, при столкновении с любыми моделями машинного обучения."
      ]
    }
  ]
}