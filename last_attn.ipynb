{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIN3U/MQ9p0WAD02PBxK4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/eng_to_jap_translator/blob/main/last_attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QQohLwxFEf0",
        "outputId": "d104842e-d8bd-4347-cf62-3832066330d3"
      },
      "source": [
        "!pip install tinysegmenter"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tinysegmenter in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffN6mQimAG0O"
      },
      "source": [
        "*Kurohashi-Kawahara Lab. has the copyright of Japanese Basic Sentence Data, and NICT MASTAR Project, Multilingual Translation Lab. has the copyright of English and Chinese Basic Sentence Data. You can use all the data under the terms of the Creative Commons Attribution 3.0 Unported license.\n",
        "     http://nlp.ist.i.kyoto-u.ac.jp/EN/?JEC%20Basic%20Sentence%20Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z76bIavR4OgJ"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from math import ceil\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate import bleu_score\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import tinysegmenter"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ-fFCDNNp8L"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EykjZGFFOgV"
      },
      "source": [
        "segmenter = tinysegmenter.TinySegmenter()"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbLZV20E6xww"
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhuE25fp8kyo"
      },
      "source": [
        "my_frame = pd.read_excel('http://nlp.ist.i.kyoto-u.ac.jp/EN/?plugin=attach&refer=JEC%20Basic%20Sentence%20Data&openfile=JEC_basic_sentence_v1-2.xls')"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF5b8Q-1-NCM"
      },
      "source": [
        "#remove Chineese column\n",
        "my_frame = my_frame.drop(['难道不会是X吗，我实在是感到怀疑。'], axis=1)\n",
        "my_frame.columns = ['index', 'jp', 'en']\n",
        "my_frame = my_frame.drop(['index'], axis=1)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WyprtJySpvT"
      },
      "source": [
        "for i in range(len(my_frame)):\n",
        "  my_frame['en'][i] = my_frame['en'][i].lower()"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "or_8lZYz-TSy",
        "outputId": "5f4cef0a-61c3-42b3-8fd2-3f8a11f381d1"
      },
      "source": [
        "my_frame"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jp</th>\n",
              "      <th>en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Xがいいなといつも思います</td>\n",
              "      <td>i always think x would be nice.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>それがあるようにいつも思います</td>\n",
              "      <td>it always seems like it is there.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>それが多すぎないかと正直思う</td>\n",
              "      <td>i honestly feel like there is too much.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
              "      <td>i think that yamada is the type everybody likes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>〜と誰かが思った</td>\n",
              "      <td>someone thought that 〜</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>チームが４人のメンバーで構成されています</td>\n",
              "      <td>the team consists of four members.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5299</th>\n",
              "      <td>彼が実際に動画を再生する</td>\n",
              "      <td>he actually plays the video.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5300</th>\n",
              "      <td>政府が銀行に公的資金をどんどん投入しました</td>\n",
              "      <td>the government injected massive public funds i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5301</th>\n",
              "      <td>レベル１の機能に下記の機能をプラスする</td>\n",
              "      <td>the following will be added to the level 1 fun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5302</th>\n",
              "      <td>彼が携帯を制服のポケットに仕舞う</td>\n",
              "      <td>he puts his cell phone into the pocket of his ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5303 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         jp                                                 en\n",
              "0             Xがいいなといつも思います                    i always think x would be nice.\n",
              "1           それがあるようにいつも思います                  it always seems like it is there.\n",
              "2            それが多すぎないかと正直思う            i honestly feel like there is too much.\n",
              "3      山田はみんなに好かれるタイプの人だと思う   i think that yamada is the type everybody likes.\n",
              "4                  〜と誰かが思った                             someone thought that 〜\n",
              "...                     ...                                                ...\n",
              "5298   チームが４人のメンバーで構成されています                 the team consists of four members.\n",
              "5299           彼が実際に動画を再生する                       he actually plays the video.\n",
              "5300  政府が銀行に公的資金をどんどん投入しました  the government injected massive public funds i...\n",
              "5301    レベル１の機能に下記の機能をプラスする  the following will be added to the level 1 fun...\n",
              "5302       彼が携帯を制服のポケットに仕舞う  he puts his cell phone into the pocket of his ...\n",
              "\n",
              "[5303 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NR5I8JtFW30",
        "outputId": "c1ea5a20-6b26-4ae6-8879-0215430624d0"
      },
      "source": [
        "segmenter.tokenize(my_frame['jp'][0])"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X', 'が', 'いい', 'な', 'といつも', '思い', 'ます']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF0YbdWtNwqG",
        "outputId": "a0805ab5-2dde-498c-9b86-0d48f18e8798"
      },
      "source": [
        "[tok.text for tok in spacy_en.tokenizer(my_frame['en'][1])]"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it', 'always', 'seems', 'like', 'it', 'is', 'there', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9jArBLD37mK",
        "outputId": "09d36ec8-ad0c-4d03-c9de-193727b55bfd"
      },
      "source": [
        "for word in spacy_en.tokenizer(my_frame['en'][1]):\n",
        "  print(word)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it\n",
            "always\n",
            "seems\n",
            "like\n",
            "it\n",
            "is\n",
            "there\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlOIJb8gCZke"
      },
      "source": [
        "valid_border = ceil(len(my_frame)*0.8)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbs43oD3RJNy"
      },
      "source": [
        "train_df = my_frame[:valid_border]\n",
        "valid_df = my_frame[valid_border:]\n",
        "len(train_df), len(valid_df)\n",
        "valid_df = valid_df.reset_index()"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3C8Z7as_8aY"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYfkUoKCA_Qx"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "# def normalizeString(s):\n",
        "#     # s = unicodeToAscii(s.lower().strip())\n",
        "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "#     return s"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Lc2S6govZK",
        "outputId": "55984e47-8472-49cb-80bf-fdb73828a393"
      },
      "source": [
        "    for index, sent in enumerate(train_df['jp']):\n",
        "      if index == 1:\n",
        "        print(index, sent)\n",
        "      pair = [str(spacy_en.tokenizer(train_df['en'][index].lower())), ' '.join(segmenter.tokenize(sent))]\n",
        "      if index == 1:\n",
        "        print(pair)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 それがあるようにいつも思います\n",
            "['it always seems like it is there.', 'それ が ある よう にいつも 思い ます']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T22fh0XBFLN"
      },
      "source": [
        "def readLangs(lang1, lang2, frame, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    # lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "    #     read().strip().split('\\n')\n",
        "    pairs = []\n",
        "    # Split every line into pairs and normalize\n",
        "    for index, sent in enumerate(frame['jp']):\n",
        "      pair = [str(spacy_en.tokenizer(frame['en'][index].lower())), ' '.join(segmenter.tokenize(sent))]\n",
        "      pairs.append(pair)\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoVdlZE5BJkD"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "        # p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEJ35nWlBOVg",
        "outputId": "90f8d340-fc3b-49bb-ce6c-8777196ff281"
      },
      "source": [
        "\n",
        "def prepareData(lang1, lang2, frame, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, frame, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('en', 'jp', my_frame, False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 5303 sentence pairs\n",
            "Trimmed to 5220 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 7329\n",
            "jp 6988\n",
            "[\"he didn't actually use the computer.\", '彼 が パソコン を 実際 に は 使用 し なかっ た']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w855I6fwvKwo",
        "outputId": "e98c9584-f43b-4a79-9cd4-a28971a40fb6"
      },
      "source": [
        "_, _, train_pairs = prepareData('en', 'jp', train_df, False)\n",
        "print(random.choice(train_pairs))\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 4243 sentence pairs\n",
            "Trimmed to 4182 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 6237\n",
            "jp 5656\n",
            "['my thinking did a 180.', '１ ８ ０ 度 自分 の 考え方 が 変わり まし た']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7B3mmr2vzrW",
        "outputId": "c3e28faf-72f0-4287-ab95-f86c7aa3bf5c"
      },
      "source": [
        "_, _, valid_pairs = prepareData('en', 'jp', valid_df, False)\n",
        "print(random.choice(train_pairs))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1060 sentence pairs\n",
            "Trimmed to 1038 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 2827\n",
            "jp 2820\n",
            "['we will spend 48 minutes in the darkness.', '私 たち は 暗闇 の 中 で ４ ８ 分 もの 時間 を 過ごす']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH-wNL4-BR_q"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Md8F4vSu-B8"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZ-64HJvLD_"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyRvQh6VvPnE"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    res = []\n",
        "    \n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbN1WpQFqSm7"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    sentence = sentence.lower()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prCzxljCvai-"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
        "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4mx4TiOvfeS"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3KM_SAzy34U"
      },
      "source": [
        "try:\n",
        "  for instance in list(tqdm._instances):\n",
        "    tqdm._decr_instances(instance)\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-LADdCNvj-4"
      },
      "source": [
        "def trainIters(encoder, decoder, epochs, print_every=500, \n",
        "               plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    # plot_losses = []\n",
        "    list_losses = []\n",
        "    loss_total = 0\n",
        "      # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    local_pairs = [tensorsFromPair(pair)\n",
        "                      for pair in train_pairs]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    \n",
        "    for e in range(epochs):\n",
        "      progress_bar = tqdm(total=epochs, desc=f'{ e }')\n",
        "      for iter in range(len(local_pairs) - 1):\n",
        "\n",
        "          training_pair = local_pairs[iter]\n",
        "          input_tensor = training_pair[0]\n",
        "          target_tensor = training_pair[1]\n",
        "\n",
        "          loss = train(input_tensor, target_tensor, encoder,\n",
        "                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "          loss_total += loss\n",
        "          plot_loss_total += loss\n",
        "          list_losses.append(loss)\n",
        "\n",
        "          progress_bar.set_postfix(loss=np.mean(list_losses[-print_every:]),\n",
        "                                  perplexity=np.exp(np.mean(list_losses[-print_every:])))\n",
        "          progress_bar.update()\n",
        "      \n",
        "      progress_bar.close()"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXJkIiK0voTu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ky1FK7KvuiM"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnuDEsCapRLv"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.6).to(device)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOkBreaXvyQO",
        "outputId": "34a7f1c5-f142-4b4a-fe1d-b2fd45ec315b"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.6).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 15)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 4181it [01:56, 35.75it/s, loss=4.63, perplexity=102]\n",
            "1: 4181it [01:57, 35.55it/s, loss=4.41, perplexity=82.7]\n",
            "2: 4181it [01:58, 35.24it/s, loss=4.23, perplexity=68.9]\n",
            "3: 4181it [02:00, 34.81it/s, loss=3.95, perplexity=51.9]\n",
            "4: 4181it [02:00, 34.64it/s, loss=3.65, perplexity=38.3]\n",
            "5: 4181it [02:01, 34.55it/s, loss=3.4, perplexity=30]\n",
            "6: 4181it [02:01, 34.32it/s, loss=3.07, perplexity=21.5]\n",
            "7: 4181it [02:02, 34.22it/s, loss=2.86, perplexity=17.5]\n",
            "8: 4181it [02:02, 34.10it/s, loss=2.59, perplexity=13.4]\n",
            "9: 4181it [02:04, 33.47it/s, loss=2.4, perplexity=11]\n",
            "10: 4181it [02:06, 33.18it/s, loss=2.14, perplexity=8.49]\n",
            "11: 4181it [02:06, 32.99it/s, loss=1.96, perplexity=7.13]\n",
            "12: 4181it [02:06, 33.00it/s, loss=1.91, perplexity=6.78]\n",
            "13: 4181it [02:09, 32.31it/s, loss=1.81, perplexity=6.13]\n",
            "14: 4181it [02:06, 33.00it/s, loss=1.7, perplexity=5.48]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0L7hpKpulC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d89706e7-ab31-4920-fc7e-3fba05d10393"
      },
      "source": [
        "phrase, tenz = evaluate(encoder1, attn_decoder1, \"I want to kill\")\n",
        "' '.join(phrase[:-1])"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'私 が が を を だ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNAUyqdWwHR-",
        "outputId": "105d9c4d-3ee3-4055-9675-eba16832bdf1"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShow(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "\n",
        "\n",
        "\n",
        "evaluateAndShow(\"I want to eat\")\n",
        "evaluateAndShow(\"I want to kill\")\n",
        "evaluateAndShow(\"I want to drink\")\n",
        "evaluateAndShow(\"I kill you\")"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = I want to eat\n",
            "output = 楽しみ が 思わず を 心から に <EOS>\n",
            "input = I want to kill\n",
            "output = 私 が が を を ん <EOS>\n",
            "input = I want to drink\n",
            "output = 私 が が を を し <EOS>\n",
            "input = I kill you\n",
            "output = 私 が が の が が まし まし まし まし まし まし まし まし まし まし まし まし まし まし\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TEWE_E4U2IL",
        "outputId": "d69ead05-2913-41e1-8702-b887283787ea"
      },
      "source": [
        "train_pairs[0]"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i always think x would be nice.', 'X が いい な といつも 思い ます']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in0FGd5swRkL"
      },
      "source": [
        "def cal_bleu_score(dataset_pairs, encoder, decoder):\n",
        "    targets = []\n",
        "    predictions = []\n",
        " \n",
        "    for i in range(len(dataset_pairs)):\n",
        "        target = dataset_pairs[i][1]\n",
        "        # train_pairs\n",
        "        predicted_words, _ = evaluate(encoder, decoder, dataset_pairs[i][0])\n",
        "        predictions.append(' '.join(predicted_words[:-1]))\n",
        "        targets.append(target)\n",
        "    print(predictions[:3])\n",
        "    print(targets[:3])\n",
        "    print(f'BLEU Score: {round(corpus_bleu(predictions, targets) * 100, 2)}')\n"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxBGxaD5RGJ-",
        "outputId": "e6337ff0-527b-4085-a3c9-db823fae842e"
      },
      "source": [
        "cal_bleu_score(train_pairs, encoder1, attn_decoder1)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['X が いい の といつも が ます ます ます', 'それ が 思い にいつも 思い 思い', 'それ が 多すぎ ない と 正直思 の 多すぎ を ない']\n",
            "['X が いい な といつも 思い ます', 'それ が ある よう にいつも 思い ます', 'それ が 多すぎ ない か と 正直思 う']\n",
            "BLEU Score: 78.97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoOsJPccRTOR",
        "outputId": "ba49f467-8304-4ecf-d376-8871c4cbad94"
      },
      "source": [
        "\n",
        "cal_bleu_score(valid_pairs, encoder1, attn_decoder1)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['新た に は に に に の に 運営 し し て ます を ます する', '各 に に に に に の が が を を し まし', 'それ が は の 多く の を を し た て い ます']\n",
            "['４月 から 個人 情報 保護 法 が 施行 さ れ まし た', '徐々 に 腎臓 の 機能 が 低下 し ます', '仕事 の 能力 が 低下 し た']\n",
            "BLEU Score: 66.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w13PUnd_Wj-R",
        "outputId": "a93361f5-490a-4dd3-b8dc-5a563d5a13b0"
      },
      "source": [
        "train_pairs[0]"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i always think x would be nice.', 'X が いい な といつも 思い ます']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxaIUmecgmA0",
        "outputId": "c81897a5-362c-4fc9-9cc0-000bcaf48265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateAndShow(\"i think it would be nice \")"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = i think it would be nice \n",
            "output = 自分 が が が に <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC42bfPbgvxT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}